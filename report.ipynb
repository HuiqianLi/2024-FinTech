{
    "cells": [
     {
      "cell_type": "markdown",
      "id": "d9a47881-27bb-4d44-8542-c0e9a0002c7c",
      "metadata": {},
      "source": [
       "## 1. 数据读取和预处理"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "a492c590-494f-4e73-98cc-350e926be30b",
      "metadata": {},
      "source": [
       "给定数据是csv的表格，包含54列数据以及标签和ID。使用pandas进行数据读取，并放在`data`中，为了避免对标签操作，先删除了标签和ID列，后续再加回来。"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "33da373c-f227-40e6-bac3-e73f95eeb718",
      "metadata": {},
      "outputs": [],
      "source": [
       "# 读取数据\n",
       "train = pd.read_csv('/work/data/train_base_data.csv')\n",
       "test = pd.read_csv('/work/data/test_data.csv')\n",
       "data = pd.concat([train, test]).reset_index(drop=True)\n",
       "\n",
       "y_label = ['ID', 'CHANNEL_A', 'CHANNEL_B', 'CHANNEL_C']\n",
       "y_data = data[y_label]\n",
       "data = data.drop(columns=y_label)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "75d9dabd-318e-4aff-8176-ebc5feb9ab1f",
      "metadata": {},
      "source": [
       "首先，通过观察确定了数据集中的字符型特征列，具体包括COL3、COL4、COL5和COL19。对于这些字符型特征，实施了`Label Encoding`，这是一种将类别标签转换为连续数值的技术，以便于后续的数值分析。\n",
       "\n",
       "对于数据集中的缺失值，在尝试了均值填充和众数填充之后，决定采用更为保守的策略，即直接将缺失值填充为`-2`。\n",
       "\n",
       "最后，将列中的最大值大于1000的加入`num_f`作为代表性的数值型特征。\n",
       "\n",
       "为了有效减少无用的数据，对方差较大和较小的进行了删除。"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "b428eb84-dc1e-4793-a468-95546eb6bc02",
      "metadata": {},
      "outputs": [],
      "source": [
       "def data_preprocessing(data, mode='train'):\n",
       "    ff = [i for i in data.columns if i not in ['ID', 'CHANNEL_A', 'CHANNEL_B', 'CHANNEL_C']]\n",
       "    # 需要特别处理的类别特征的列名\n",
       "    cat_f = ['COL3', 'COL4', 'COL5', 'COL19']\n",
       "    num_f = []  # 存储数值型特征的列名\n",
       "    for f in tqdm(ff):\n",
       "        data[f] = data[f].fillna(-2)\n",
       "        data[f] = data[f].astype('str')\n",
       "        data[f] = data[f].apply(lambda x: x.replace(' ', '-1'))\n",
       "        if f not in cat_f:\n",
       "            data[f] = data[f].astype('float')\n",
       "        else:\n",
       "            data[f] = data[f].astype('str')\n",
       "            # 对类别特征进行Label Encoding\n",
       "            if data[f].dtype == 'object':\n",
       "                lb = LabelEncoder()\n",
       "                data[f] = lb.fit_transform(data[f])\n",
       "            else:\n",
       "                grade_dict = {chr(i): i-96 for i in range(97, 123)}\n",
       "                data[f] = data[f].map(grade_dict)\n",
       "        if data[f].max()>1000 and mode != 'test':\n",
       "            num_f.append(f)\n",
       "\n",
       "    # 去掉低方差、高方差特征\n",
       "    if mode != 'test':\n",
       "        data, num_f, ff, cat_f = remove_features(data, num_f, ff, cat_f)\n",
       "    \n",
       "    return data, num_f, ff, cat_f"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "7b10b432-9fad-4877-a109-06ea0e7cd0c8",
      "metadata": {},
      "source": [
       "## 2. 特征工程"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "b7f9e55a-3869-4b23-a678-bc799d65f9a5",
      "metadata": {},
      "source": [
       "数据预处理后，下一步进行特征工程。分别尝试了：\n",
       "1. 偏离值特征\n",
       "2. 数值和类别特征交叉\n",
       "3. 数值特征加减乘除交叉\n",
       "4. 数值特征做 max, min, mean, std\n",
       "\n",
       "实验发现，偏离值特征的作用不大，而数值特征的两个处理作用相似，故最终保留了计算量较小的“数值特征做 max, min, mean, std”，最终的特征工程如下："
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbb65237-a6e3-4615-b464-8fd24696f3c9",
      "metadata": {},
      "outputs": [],
      "source": [
       "def feature_engineering(data, num_f, ff, cat_f, mode='train'):\n",
       "    print('the num of ff num_f cat_f ', len(ff), len(num_f), len(cat_f))  # 33 0 1\n",
       "    # for group in tqdm(cat_f):\n",
       "    #     for i, feature in enumerate(ff, start=1):  # 添加序号计数，从1开始\n",
       "    #         if feature not in cat_f:\n",
       "    #             tmp = data.groupby(group)[feature].agg(['mean', 'std', 'max', 'min', 'sum']).reset_index()\n",
       "    #             tmp = pd.merge(data, tmp, on=group, how='left')\n",
       "    #             # 创建新的特征，表示相对于组内统计的偏差\n",
       "    #             data['{}-mean_gb_{}'.format(feature, group)] = data[feature] - tmp['mean']\n",
       "    #             data['{}-min_gb_{}'.format(feature, group)] = data[feature] - tmp['min']\n",
       "    #             data['{}-max_gb_{}'.format(feature, group)] = data[feature] - tmp['max']\n",
       "    #             data['{}/sum_gb_{}'.format(feature, group)] = data[feature] / tmp['sum']   \n",
       "    \n",
       "    # 数值型特征和类别特征之间的交叉\n",
       "    for i in tqdm(range(len(num_f))):\n",
       "        for j in range(i + 1, len(num_f)):\n",
       "            for cat in cat_f[1:]:\n",
       "                f1 = ff[i]\n",
       "                f2 = ff[j]\n",
       "                data[f'{f1}_{f2}_log_{cat}'] = (np.log1p(data[f1]) - np.log1p(data[f2])) * data[cat]\n",
       "                data[f'{f1}+{f2}_log_{cat}'] = (np.log1p(data[f1]) + np.log1p(data[f2])) * data[cat]\n",
       "                data[f'{f1}*{f2}_log_{cat}'] = (np.log1p(data[f1]) * np.log1p(data[f2])) * data[cat]\n",
       "                data[f'{f1}/{f2}_log_{cat}'] = (np.log1p(data[f1]) / np.log1p(data[f2])) * data[cat]\n",
       "                data[f'{f2}/{f1}_log_{cat}'] = (np.log1p(data[f2]) / np.log1p(data[f1])) * data[cat]\n",
       "\n",
       "                data[f'{f1}_{f2}_log_{cat}_'] = (np.log1p(data[f1]) - np.log1p(data[f2])) / data[cat]\n",
       "                data[f'{f1}+{f2}_log_{cat}_'] = (np.log1p(data[f1]) + np.log1p(data[f2])) / data[cat]\n",
       "                data[f'{f1}*{f2}_log_{cat}_'] = (np.log1p(data[f1]) * np.log1p(data[f2])) / data[cat]\n",
       "                data[f'{f1}/{f2}_log_{cat}_'] = (np.log1p(data[f1]) / np.log1p(data[f2])) / data[cat]\n",
       "                data[f'{f2}/{f1}_log_{cat}_'] = (np.log1p(data[f2]) / np.log1p(data[f1])) / data[cat]\n",
       "\n",
       "    # # 数值型特征之间的加减乘除交叉\n",
       "    # for i in tqdm(range(len(num_f))):\n",
       "    #     for j in range(i + 1, len(num_f)):\n",
       "    #         f1 = ff[i]\n",
       "    #         f2 = ff[j]\n",
       "    #         data[f'{f1}_{f2}'] = data[f1] - data[f2]\n",
       "    #         data[f'{f1}+{f2}'] = data[f1] + data[f2]\n",
       "    #         data[f'{f1}*{f2}'] = data[f1] * data[f2]\n",
       "    #         data[f'{f1}/{f2}'] = data[f1] / data[f2]\n",
       "    #         data[f'{f2}/{f1}'] = data[f2] / data[f1]\n",
       "\n",
       "    # 数值特征做 max, min, mean, std\n",
       "    for i in tqdm(range(len(num_f))):\n",
       "        f = ff[i]\n",
       "        data[f'{f}_max'] = data[f].max()\n",
       "        data[f'{f}_min'] = data[f].min()\n",
       "        mean_series = data[f].mean()\n",
       "        std_series = data[f].std()\n",
       "        ptp_series = data[f].max() - data[f].min()  # 计算峰峰值\n",
       "        data[f'{f}_mean'] = mean_series\n",
       "        data[f'{f}_std'] = std_series\n",
       "        data[f'{f}_ptp'] = ptp_series\n",
       "\n",
       "    return data"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "2357cad1-2ae0-48f3-a2c0-a901d0f09138",
      "metadata": {},
      "source": [
       "## 3. 模型全特征训练"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "5e429d4d-3a71-472d-a07d-bbcacd80b0d5",
      "metadata": {},
      "source": [
       "数据和标签准备好之后，开始进行模型的训练。我们的任务是3个二分类问题，故构建三个模型，选择`lightgbm`作为模型，使用5折交叉验证。boosting类型选择`gbdt`，目标函数选择`binary`，评价指标选择`auc`。为了方便后续的特征选择，我们需要保存每个特征的重要性，以便后续的特征选择。"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "df05cf9c-9815-4cb6-bc5a-c35b49b31f4b",
      "metadata": {},
      "outputs": [],
      "source": [
       "# 训练模型\n",
       "def train_model(X_train, X_test, features, y, threshold_, params, seed=2024, save_model=False, model_path='model.txt', kf=10):\n",
       "    feat_imp_df = pd.DataFrame({'feat': features, 'imp': 0})    # 存储特征名称和它们的重要性\n",
       "    KF = StratifiedKFold(n_splits=kf, random_state=seed, shuffle=True)   # 5折交叉验证\n",
       "    # 初始化保存每个折的分数列表\n",
       "    score_lists = []\n",
       "    \n",
       "    oof_lgb = np.zeros(len(X_train))    # 初始化1个任务的oof预测结果\n",
       "    predictions_lgb = np.zeros(len(X_test))  # 测试集的预测结果，1个任务\n",
       "\n",
       "    for fold_, (trn_idx, val_idx) in enumerate(KF.split(X_train.values, y.values)):\n",
       "        print(\"[fold n°{}]\".format(fold_ + 1))\n",
       "        trn_data = lgb.Dataset(X_train.iloc[trn_idx][features], label=y.iloc[trn_idx])\n",
       "        val_data = lgb.Dataset(X_train.iloc[val_idx][features], label=y.iloc[val_idx])\n",
       "\n",
       "        clf = lgb.train(params, \n",
       "                        trn_data, \n",
       "                        valid_sets=[trn_data, val_data], \n",
       "                        verbose_eval=100)\n",
       "        \n",
       "        # model_lgb = lgb.LGBMClassifier(objective='binary', max_depth=3, num_leaves=50,\n",
       "        #                     n_estimators=5000,\n",
       "        #                     min_child_samples=18, min_child_weight=0.001,\n",
       "        #                     feature_fraction=0.6, bagging_fraction=0.5,\n",
       "        #                     metric='auc', )\n",
       "        # params_test={\n",
       "        #         'learning_rate=': [0.5, 0.1, 0.05, 0.01],\n",
       "        #     }\n",
       "        # gsearch = GridSearchCV(estimator=model_lgb, param_grid=params_test, scoring='f1_micro', cv=5, verbose=1, n_jobs=4)\n",
       "        # gsearch.fit(X_train.iloc[trn_idx][features], y.iloc[trn_idx])\n",
       "        # print(gsearch.best_params_, gsearch.best_score_)\n",
       "\n",
       "        oof_lgb[val_idx] = clf.predict(X_train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
       "        predictions_lgb += clf.predict(X_test[features], num_iteration=clf.best_iteration) / KF.n_splits\n",
       "        feat_imp_df['imp'] += clf.feature_importance() / KF.n_splits\n",
       "        score_lists.append(f1_score(y.iloc[val_idx], [1 if i >= threshold_ else 0 for i in oof_lgb[val_idx]]))\n",
       "\n",
       "\n",
       "    # 打印每个任务的评估指标\n",
       "    print(\"AUC score: {}\".format(roc_auc_score(y, oof_lgb)))\n",
       "    print(\"F1 score: {}\".format(f1_score(y, [1 if i >= threshold_ else 0 for i in oof_lgb])))\n",
       "    print(\"Precision score: {}\".format(precision_score(y, [1 if i >= threshold_ else 0 for i in oof_lgb])))\n",
       "    print(\"Recall score: {}\".format(recall_score(y, [1 if i >= threshold_ else 0 for i in oof_lgb])))\n",
       "    print(\"F1 mean: {}\".format(np.mean(score_lists)))\n",
       "\n",
       "    # # 假设oof_lgb是你的模型输出的概率，y是真实标签\n",
       "    # thresholds = np.linspace(0, threshold_+0.1, 100)  # 生成一系列可能的阈值\n",
       "    # best_threshold = 0\n",
       "    # best_f1 = 0\n",
       "\n",
       "    # for threshold in thresholds:\n",
       "    #     y_pred = [1 if i >= threshold else 0 for i in oof_lgb]\n",
       "    #     current_f1 = f1_score(y, y_pred)\n",
       "    #     if current_f1 > best_f1:\n",
       "    #         best_f1 = current_f1\n",
       "    #         best_threshold = threshold\n",
       "\n",
       "    # print(\"Best F1 score: {}\".format(best_f1))\n",
       "    # print(\"Best threshold: {}\".format(best_threshold))\n",
       "\n",
       "    # # 使用最佳阈值计算其他指标\n",
       "    # y_pred_best = [1 if i >= best_threshold else 0 for i in oof_lgb]\n",
       "    # print(\"AUC score: {}\".format(roc_auc_score(y, oof_lgb)))\n",
       "    # print(\"F1 score with best threshold: {}\".format(f1_score(y, y_pred_best)))\n",
       "    # print(\"Precision score with best threshold: {}\".format(precision_score(y, y_pred_best)))\n",
       "    # print(\"Recall score with best threshold: {}\".format(recall_score(y, y_pred_best)))\n",
       "\n",
       "    if save_model:\n",
       "        booster = lgb.train(params, trn_data, valid_sets=[trn_data, val_data], verbose_eval=100)\n",
       "        booster.save_model(model_path)  # 保存模型到文件\n",
       "    \n",
       "    # 返回特征重要性、每个任务的oof预测结果和测试集的预测结果\n",
       "    return feat_imp_df, oof_lgb, predictions_lgb"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "5187cb18-8320-49b2-976b-6d04fa5357f7",
      "metadata": {},
      "source": [
       "由于最终评价指标为F1，计算精确率和召回率时，**阈值的选择**尤为重要。我先预设阈值都为0.5，对模型进行训练推理，计算出推理到的测试集中预测值的均值作为阈值，即`[0.05,0.25,0.05]`，但是这样计算仍有改进的空间。在后续训练时，我在(0, threshold_+0.1)中均匀采样100次，分别计算F1指标，选择F1最高的作为最终阈值(见代码注释的地方)，结合线上预测结果，最终选定阈值为`[0.06, 0.2575757575757576, 0.03333333333333334]`。"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "3aba67b0-af4c-4bf2-bdfd-321e83beeb97",
      "metadata": {},
      "source": [
       "## 4. 重要特征训练"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "aa52351a-9b77-43db-92be-741ce4d53047",
      "metadata": {},
      "source": [
       "模型全特征训练完，保存下了特征重要性，然后我们可以根据特征重要性筛选特征。我筛选了重要性大于`0.05`的特征，然后再次训练模型，这次进行10折交叉验证，充分保证模型的质量，训练结束后把3个模型分别保存下来。"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "d31f4708-cb53-4bfd-ad26-d5ea476f6fb5",
      "metadata": {},
      "outputs": [],
      "source": [
       "length = [0 for _ in range(3)]\n",
       "# 筛选大于0.05的特征\n",
       "for i in range(3):\n",
       "    feat_imp_df_i = feat_imp_df[i]\n",
       "    features_ = feat_imp_df_i[feat_imp_df_i['imp'] > 0.05]['feat'].to_list()\n",
       "    length[i] = len(features_)\n",
       "pred = [[] for _ in range(3)]\n",
       "oof = [[] for _ in range(3)]\n",
       "for i in range(3):\n",
       "    print('[Channel {}]'.format(i))\n",
       "    feat_imp_df_i = feat_imp_df[i]\n",
       "    pred[i], oof[i] = mean_fusion(train, test, feat_imp_df_i, y.iloc[i], threshold[i], length[i], params[i], model_path='model_{}.txt'.format(i))\n"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "e019513e-5fd5-452b-af9c-07bb82f41e1b",
      "metadata": {},
      "source": [
       "## 5. 测试"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "128310ce-1bb7-4b1c-af81-f907168850d7",
      "metadata": {},
      "source": [
       "训练完毕后，编写测试代码，按照以上进行数据预处理和特征工程，读取保存好的重要性特征，选择训练时用到的特征进行预测。"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ef968f1-79c2-4704-9cf1-d634a5f167b4",
      "metadata": {},
      "outputs": [],
      "source": [
       "# 加载特征重要性\n",
       "with open('feat_imp_a.pkl', 'rb') as f:\n",
       "    feat_imp_df = pickle.load(f)\n",
       "\n",
       "pred = [[] for _ in range(3)]\n",
       "for i in range(3):\n",
       "    # 加载保存的模型\n",
       "    booster = lgb.Booster(model_file='/work/model_{}.txt'.format(i))\n",
       "    feat_imp_df_i = feat_imp_df[i]\n",
       "    features = feat_imp_df_i.sort_values(['imp'])[-length[i]:]['feat'].to_list()\n",
       "    pred[i] = booster.predict(test[features], num_iteration=booster.best_iteration)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "f0e2da8c-53d7-4aaa-9a30-6a3837b015ab",
      "metadata": {},
      "source": [
       "最后将结果写进`'/work/output.csv'`文件中。"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "24359b2f-ae00-44ae-b847-1f84fd3a6493",
      "metadata": {},
      "outputs": [],
      "source": [
       "pred_np = np.array(pred)\n",
       "# # 分别计算一下三个通道的预测值的均值\n",
       "# A_mean = np.mean(pred_np[0])\n",
       "# B_mean = np.mean(pred_np[1])\n",
       "# C_mean = np.mean(pred_np[2])\n",
       "# print(A_mean, B_mean, C_mean)\n",
       "\n",
       "test['CHANNEL_A'] = np.where(pred_np[0] >= threshold[0], 1, 0)\n",
       "test['CHANNEL_B'] = np.where(pred_np[1] >= threshold[1], 1, 0)\n",
       "test['CHANNEL_C'] = np.where(pred_np[2] >= threshold[2], 1, 0)\n",
       "\n",
       "import csv\n",
       "with open('/work/output.csv', newline='', mode='w') as outputFile:\n",
       "    fieldnames = ['ID', 'CHANNEL_A', 'CHANNEL_B', 'CHANNEL_C']\n",
       "    writer = csv.DictWriter(outputFile, fieldnames=fieldnames)\n",
       "    writer.writerow({'ID': 'ID', 'CHANNEL_A': 'CHANNEL_A', 'CHANNEL_B': 'CHANNEL_B', 'CHANNEL_C': 'CHANNEL_C'})\n",
       "    for index, row in test[['ID', 'CHANNEL_A', 'CHANNEL_B', 'CHANNEL_C']].iterrows():\n",
       "        writer.writerow({'ID': row['ID'], 'CHANNEL_A': row['CHANNEL_A'], 'CHANNEL_B': row['CHANNEL_B'], 'CHANNEL_C': row['CHANNEL_C']})\n",
       "print(test[['ID', 'CHANNEL_A']].head())"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "1d027a74-3bd9-4330-a045-b3f52aff1075",
      "metadata": {
       "tags": []
      },
      "source": [
       "## 6. LGB模型进行调参"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "edf99eb2-6a13-41c0-a78d-80520bf36662",
      "metadata": {},
      "source": [
       "> 上述需要训练3个模型，三个模型的参数可能是不一样的，而且参数的选择对结果也有影响，故尝试了网格搜索进行调参，但是效果并不理想，这里先把调参过程列下。"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "80922ccb-fa84-4e99-9fde-b0df96902a2a",
      "metadata": {},
      "source": [
       "先把学习率先定一个较高的值，这里取 `learning_rate = 0.1`，其次确定估计器`boosting_type`的类型，默认选`gbd`。"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "2cc74ef6-6c29-42b6-ab2e-fdefbb98fca9",
      "metadata": {},
      "source": [
       "为了确定估计器的数目，也就是boosting迭代的次数，参数名为`num_boost_round`，先将该参数设成一个较大的数5000，然后在设置了早停`early_stopping_round=200`，避免过拟合。"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "371891e4-95f3-4a45-b878-5115cb7e8a5d",
      "metadata": {},
      "source": [
       "接下来进行其他参数调优，引入`sklearn`中的`GridSearchCV()`函数进行网格搜索。"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "efb5db18-167c-41c0-a9d5-222107f4b32e",
      "metadata": {},
      "source": [
       "1. 首先调整`num_leaves`与`max_depth`"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "fff132c5-e29c-4a0d-bc0d-61989503848d",
      "metadata": {},
      "outputs": [],
      "source": [
       "from sklearn.model_selection import GridSearchCV\n",
       "model_lgb = lgb.LGBMClassifier(objective='binary',num_leaves=2**6,\n",
       "                      learning_rate=0.1, n_estimators=5000,\n",
       "                      metric='auc')\n",
       "params_test={\n",
       "    'max_depth': range(3,8,2),\n",
       "    'num_leaves':range(50, 170, 30)\n",
       "}\n",
       "gsearch = GridSearchCV(estimator=model_lgb, param_grid=params_test, scoring='f1_micro', cv=5, verbose=1, n_jobs=4)\n",
       "gsearch.fit(X_train.iloc[trn_idx][features], y.iloc[trn_idx])\n",
       "print(gsearch.best_params_, gsearch.best_score_)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "27b276aa-722a-43f2-a06f-b31eea1a646c",
      "metadata": {},
      "source": [
       "打印了以下运行结果，这里运行了12个参数组合，得到的最优解是在`max_depth`为3/5，`num_leaves`为50的情况下，三行对应三个模型"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaa7a97a-25a5-44e8-b942-8687b3bd09ad",
      "metadata": {},
      "outputs": [],
      "source": [
       "{'max_depth': 5, 'num_leaves': 50} 0.96\n",
       "{'max_depth': 5, 'num_leaves': 50} 0.672\n",
       "{'max_depth': 3, 'num_leaves': 50} 0.984"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "91e397d9-099d-48f7-856d-b1b0cd95de2b",
      "metadata": {},
      "source": [
       "2. 调整`min_data_in_leaf`和`min_sum_hessian_in_leaf`"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b2c66c6-82a5-4747-886f-81ef88d7b4e9",
      "metadata": {},
      "outputs": [],
      "source": [
       "model_lgb = lgb.LGBMClassifier(objective='binary', max_depth=3, num_leaves=50,\n",
       "                      learning_rate=0.1, n_estimators=5000,\n",
       "                      metric='auc', )\n",
       "params_test={\n",
       "    'min_child_samples': [18, 19, 20, 21, 22],\n",
       "    'min_child_weight':[0.001, 0.002]\n",
       "}\n",
       "gsearch = GridSearchCV(estimator=model_lgb, param_grid=params_test, scoring='f1_micro', cv=5, verbose=1, n_jobs=4)\n",
       "gsearch.fit(X_train.iloc[trn_idx][features], y.iloc[trn_idx])\n",
       "print(gsearch.best_params_, gsearch.best_score_)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b4ee861-106d-4f8f-88b8-8be655f5f374",
      "metadata": {},
      "outputs": [],
      "source": [
       "Fitting 5 folds for each of 10 candidates, totalling 50 fits"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "22c2bf79-d63a-4554-8a29-336baecd0115",
      "metadata": {},
      "source": [
       "可以看到，`min_data_in_leaf`的最优值为18，而`min_sum_hessian_in_leaf`为0.01。也就是这两个参数`min_data_in_leaf`和 `min_sum_hessian_in_leaf`。"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6e16c3e-ab06-4c9f-800f-0ea2ccdb04a0",
      "metadata": {},
      "outputs": [],
      "source": [
       "{'min_child_samples': 18, 'min_child_weight': 0.001} 0.952\n",
       "{'min_child_samples': 19, 'min_child_weight': 0.001} 0.676\n",
       "{'min_child_samples': 18, 'min_child_weight': 0.001} 0.984"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "0c546b3c-448d-4868-ae7b-3745d39d230f",
      "metadata": {},
      "source": [
       "3. `feature_fraction`参数来进行特征的子抽样，`bagging_fraction`和`bagging_freq`相当于subsample样本采样"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc4b937d-2bda-4eb5-bfce-1513374791d7",
      "metadata": {},
      "outputs": [],
      "source": [
       "model_lgb = lgb.LGBMClassifier(objective='binary', max_depth=3, num_leaves=50,\n",
       "                      learning_rate=0.1, n_estimators=5000,\n",
       "                      min_child_samples=18, min_child_weight=0.001,\n",
       "                      metric='auc', )\n",
       "params_test={\n",
       "        'feature_fraction': [0.5, 0.6, 0.7, 0.8, 0.9],\n",
       "        'bagging_fraction': [0.6, 0.7, 0.8, 0.9, 1.0]\n",
       "    }\n",
       "gsearch = GridSearchCV(estimator=model_lgb, param_grid=params_test, scoring='f1_micro', cv=5, verbose=1, n_jobs=4)\n",
       "gsearch.fit(X_train.iloc[trn_idx][features], y.iloc[trn_idx])\n",
       "print(gsearch.best_params_, gsearch.best_score_)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "910fb01c-23f2-4ec0-b9e7-e78b1df0449f",
      "metadata": {},
      "outputs": [],
      "source": [
       "Fitting 5 folds for each of 25 candidates, totalling 125 fits"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "0efe366f-fce6-42e1-b8de-1975093ce035",
      "metadata": {},
      "source": [
       "从这里可以看出来，`bagging_feaction``和feature_fraction`的理想值分别是0.6和0.5/0.8。"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdf51355-f651-45a6-b710-6cda1656581f",
      "metadata": {},
      "outputs": [],
      "source": [
       "{'bagging_fraction': 0.6, 'feature_fraction': 0.5} 0.952\n",
       "{'bagging_fraction': 0.6, 'feature_fraction': 0.8} 0.6679999999999999\n",
       "{'bagging_fraction': 0.6, 'feature_fraction': 0.5} 0.984"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "7a7e7acb-adce-4b04-b02a-002e62c591c9",
      "metadata": {},
      "source": [
       "4. 降低`learning_rate`"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f4b1fe9-7e36-496f-8425-b4db40c21135",
      "metadata": {},
      "outputs": [],
      "source": [
       "model_lgb = lgb.LGBMClassifier(objective='binary', max_depth=3, num_leaves=50,\n",
       "                     n_estimators=5000,\n",
       "                      min_child_samples=18, min_child_weight=0.001,\n",
       "                      feature_fraction=0.6, bagging_fraction=0.5,\n",
       "                      metric='auc', )\n",
       "params_test={\n",
       "        'learning_rate=': [0.5, 0.1, 0.05, 0.01],\n",
       "    }\n",
       "gsearch = GridSearchCV(estimator=model_lgb, param_grid=params_test, scoring='f1_micro', cv=5, verbose=1, n_jobs=4)\n",
       "gsearch.fit(X_train.iloc[trn_idx][features], y.iloc[trn_idx])\n",
       "print(gsearch.best_params_, gsearch.best_score_)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "a23e9c6d-65c7-4772-85ec-9db2d672a123",
      "metadata": {},
      "source": [
       "这个学习率竟然是0.5最好。"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "f10dd995-87fd-4581-8495-4576a660431a",
      "metadata": {},
      "source": [
       "## 7. 模型再训练"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "685b5665-b2fa-41ff-84a8-1830276f0668",
      "metadata": {},
      "source": [
       "在测试时，我尝试把保存的重要性特征输出进行查看（如下代码展示），发现这些特征的列名都是原本数据的列，特征工程似乎不起作用。所以我想尝试只选择这些重要性列，再次从头进行训练，数据换为只包含这些列的数据。"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 4,
      "id": "22c4d8cd-0448-4a4e-b15c-ca519b6d97ee",
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "['COL32', 'COL54', 'COL12', 'COL27', 'COL46', 'COL28', 'COL37', 'COL14', 'COL13', 'COL39', 'COL43', 'COL3', 'COL31', 'COL21', 'COL25', 'COL47', 'COL45', 'COL6', 'COL29', 'COL36', 'COL38', 'COL35', 'COL52', 'COL42', 'COL24', 'COL18', 'COL15', 'COL51', 'COL17', 'COL2', 'COL16', 'COL9', 'COL10', 'COL1']\n",
         "['COL28', 'COL12', 'COL54', 'COL27', 'COL32', 'COL13', 'COL3', 'COL39', 'COL29', 'COL6', 'COL14', 'COL46', 'COL21', 'COL47', 'COL43', 'COL38', 'COL42', 'COL35', 'COL16', 'COL17', 'COL45', 'COL25', 'COL52', 'COL10', 'COL31', 'COL37', 'COL2', 'COL9', 'COL24', 'COL36', 'COL51', 'COL15', 'COL18', 'COL1']\n",
         "['COL28', 'COL54', 'COL12', 'COL27', 'COL32', 'COL39', 'COL3', 'COL46', 'COL6', 'COL47', 'COL16', 'COL13', 'COL25', 'COL14', 'COL21', 'COL10', 'COL17', 'COL38', 'COL45', 'COL31', 'COL24', 'COL52', 'COL29', 'COL43', 'COL42', 'COL2', 'COL9', 'COL37', 'COL1', 'COL15', 'COL35', 'COL18', 'COL51', 'COL36']\n"
        ]
       }
      ],
      "source": [
       "import pickle\n",
       "\n",
       "with open('/work/num_f.txt', 'r') as f:\n",
       "    lines = f.readlines()  # 读取所有行到一个列表中\n",
       "    num_f = eval(lines[0])\n",
       "    ff = eval(lines[1])\n",
       "    cat_f = eval(lines[2])\n",
       "    length = eval(lines[3])\n",
       "        \n",
       "with open('/work/feat_imp_a.pkl', 'rb') as f:\n",
       "    feat_imp_df = pickle.load(f)\n",
       "for i in range(3):\n",
       "    feat_imp_df_i = feat_imp_df[i]\n",
       "    features = feat_imp_df_i.sort_values(['imp'])[-length[i]:]['feat'].to_list()\n",
       "    # 打印这些特征的列,发现全是原本的列名,说明特征工程无效,故设计了第二次模型训练,即run_B.py\n",
       "    print(features)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "f6d95943-6502-4149-8a51-976d1cc07ea6",
      "metadata": {},
      "source": [
       "重新训练过后，选出来的重要性特征列名如下。经实验尝试，发现好像再训练也没什么作用。训练代码和测试代码见`run_B.py`和`test_B.py`。"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b4c8e1c3-6a57-4c1f-88bd-899fb327ae69",
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "['COL17', 'COL51-max_gb_COL3', 'COL15-max_gb_COL3', 'COL18-mean_gb_COL3', 'COL39-min_gb_COL3', 'COL2/sum_gb_COL3', 'COL43-min_gb_COL3', 'COL36-max_gb_COL3', 'COL31-max_gb_COL3', 'COL42', 'COL37-max_gb_COL3', 'COL17-min_gb_COL3', 'COL24/sum_gb_COL3', 'COL16-min_gb_COL3', 'COL18-max_gb_COL3', 'COL10/sum_gb_COL3', 'COL52', 'COL51-mean_gb_COL3', 'COL51-min_gb_COL3', 'COL38', 'COL51/sum_gb_COL3', 'COL2-max_gb_COL3', 'COL9-min_gb_COL3', 'COL1-min_gb_COL3', 'COL1-mean_gb_COL3', 'COL1/sum_gb_COL3', 'COL10-mean_gb_COL3', 'COL16-max_gb_COL3', 'COL24', 'COL10-min_gb_COL3', 'COL1-max_gb_COL3', 'COL16', 'COL9', 'COL16-mean_gb_COL3', 'COL2-mean_gb_COL3', 'COL9-max_gb_COL3', 'COL1', 'COL10', 'COL9-mean_gb_COL3', 'COL17-mean_gb_COL3', 'COL2-min_gb_COL3', 'COL2']\n",
         "['COL13', 'COL25-mean_gb_COL3', 'COL29/sum_gb_COL3', 'COL31-max_gb_COL3', 'COL42-max_gb_COL3', 'COL38-mean_gb_COL3', 'COL10-mean_gb_COL3', 'COL38/sum_gb_COL3', 'COL47', 'COL43', 'COL16-mean_gb_COL3', 'COL24/sum_gb_COL3', 'COL38', 'COL1-min_gb_COL3', 'COL10-max_gb_COL3', 'COL9/sum_gb_COL3', 'COL18-max_gb_COL3', 'COL37/sum_gb_COL3', 'COL25', 'COL17-mean_gb_COL3', 'COL2-max_gb_COL3', 'COL1/sum_gb_COL3', 'COL37-max_gb_COL3', 'COL9-max_gb_COL3', 'COL37-mean_gb_COL3', 'COL17-max_gb_COL3', 'COL24-max_gb_COL3', 'COL18-min_gb_COL3', 'COL24-mean_gb_COL3', 'COL51/sum_gb_COL3', 'COL52-mean_gb_COL3', 'COL42/sum_gb_COL3', 'COL16', 'COL31-mean_gb_COL3', 'COL15-min_gb_COL3', 'COL35', 'COL36-max_gb_COL3', 'COL51-mean_gb_COL3', 'COL52', 'COL1-max_gb_COL3', 'COL17', 'COL31', 'COL15-max_gb_COL3', 'COL2-min_gb_COL3', 'COL35/sum_gb_COL3', 'COL51-max_gb_COL3', 'COL42', 'COL36-mean_gb_COL3', 'COL29-mean_gb_COL3', 'COL36/sum_gb_COL3', 'COL10', 'COL24', 'COL9-mean_gb_COL3', 'COL18-mean_gb_COL3', 'COL37', 'COL36', 'COL35-mean_gb_COL3', 'COL2', 'COL51', 'COL18', 'COL15-mean_gb_COL3', 'COL9', 'COL15', 'COL1-mean_gb_COL3', 'COL2-mean_gb_COL3', 'COL1']\n",
         "['COL29-max_gb_COL3', 'COL43-min_gb_COL3', 'COL47-max_gb_COL3', 'COL29', 'COL31', 'COL15-min_gb_COL3', 'COL29/sum_gb_COL3', 'COL35-max_gb_COL3', 'COL14-max_gb_COL3', 'COL18/sum_gb_COL3', 'COL10/sum_gb_COL3', 'COL2/sum_gb_COL3', 'COL42/sum_gb_COL3', 'COL32-mean_gb_COL3', 'COL24', 'COL9-max_gb_COL3', 'COL14-mean_gb_COL3', 'COL42', 'COL38', 'COL42-max_gb_COL3', 'COL2-max_gb_COL3', 'COL27-mean_gb_COL3', 'COL31/sum_gb_COL3', 'COL17-max_gb_COL3', 'COL14/sum_gb_COL3', 'COL14', 'COL38-max_gb_COL3', 'COL13-mean_gb_COL3', 'COL42-min_gb_COL3', 'COL47-mean_gb_COL3', 'COL2-min_gb_COL3', 'COL25/sum_gb_COL3', 'COL13/sum_gb_COL3', 'COL2', 'COL43/sum_gb_COL3', 'COL1', 'COL42-mean_gb_COL3', 'COL43', 'COL51-max_gb_COL3', 'COL9/sum_gb_COL3', 'COL16-mean_gb_COL3', 'COL52-mean_gb_COL3', 'COL10-mean_gb_COL3', 'COL13', 'COL1-max_gb_COL3', 'COL52/sum_gb_COL3', 'COL31-mean_gb_COL3', 'COL38-mean_gb_COL3', 'COL38/sum_gb_COL3', 'COL1/sum_gb_COL3', 'COL43-max_gb_COL3', 'COL35', 'COL17-mean_gb_COL3', 'COL36', 'COL37-max_gb_COL3', 'COL43-mean_gb_COL3', 'COL24-mean_gb_COL3', 'COL24/sum_gb_COL3', 'COL37', 'COL9-mean_gb_COL3', 'COL35-min_gb_COL3', 'COL18', 'COL9', 'COL37/sum_gb_COL3', 'COL18-mean_gb_COL3', 'COL37-mean_gb_COL3', 'COL36-max_gb_COL3', 'COL35/sum_gb_COL3', 'COL51/sum_gb_COL3', 'COL36-mean_gb_COL3', 'COL18-max_gb_COL3', 'COL15', 'COL51', 'COL1-mean_gb_COL3', 'COL51-mean_gb_COL3', 'COL15-mean_gb_COL3', 'COL15-max_gb_COL3', 'COL36/sum_gb_COL3', 'COL2-mean_gb_COL3', 'COL29-mean_gb_COL3', 'COL35-mean_gb_COL3']\n"
        ]
       }
      ],
      "source": [
       "import pickle\n",
       "\n",
       "with open('/work/num_f_b.txt', 'r') as f:\n",
       "    lines = f.readlines()  # 读取所有行到一个列表中\n",
       "    num_f = eval(lines[0])\n",
       "    ff = eval(lines[1])\n",
       "    cat_f = eval(lines[2])\n",
       "    length = eval(lines[3])\n",
       "        \n",
       "with open('/work/feat_imp_b.pkl', 'rb') as f:\n",
       "    feat_imp_df = pickle.load(f)\n",
       "for i in range(3):\n",
       "    feat_imp_df_i = feat_imp_df[i]\n",
       "    features = feat_imp_df_i.sort_values(['imp'])[-length[i]:]['feat'].to_list()\n",
       "    # 打印这些特征的列,发现全是原本的列名,说明特征工程无效,故设计了第二次模型训练,即run_B.py\n",
       "    print(features)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "4891812f-780e-46d0-b725-e0feba957ed6",
      "metadata": {},
      "outputs": [],
      "source": []
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }
   